"""
Ad Serving API Server
Serves personalized ads generated by the v0 pipeline to the Chrome extension.

Usage:
    python ad_server.py
    
Endpoints:
    GET  /api/ad/<user_id>           - Get best ad for user
    GET  /api/ads/<user_id>          - Get all ad variants for user
    POST /api/generate/<user_id>     - Trigger pipeline for user (with context card)
    GET  /api/users                  - List users with cached ads
    GET  /health                     - Health check
"""

import json
import os
import threading
from supabase_client import fetch_relevant_ads
from pathlib import Path
from datetime import datetime
from glob import glob
from flask import Flask, jsonify, request
from flask_cors import CORS
from temp_cache import (
    compute_ad_key,
    format_best_variant_for_cache,
    load_queue_state,
    replace_queue,
    append_ads_to_queue,
    pop_next_ad,
    queue_size,
    cache_file_path,
)

app = Flask(__name__)
CORS(app)  # Enable CORS for Chrome extension access

# Directory where pipeline outputs are stored
OUTPUT_DIR = Path(__file__).parent

# In-memory cache: user_id -> {data, timestamp}
ads_cache = {}

# Ad rotation tracking: user_id -> {shown_keys: set, last_key: str, last_index: int}
# Tracks which ORIGINAL ADS (by content key) have been shown
# We remix each original ad, pick best variant, serve it, then move to next original ad
ad_rotation = {}

# Ranked ads cache: user_id -> {ranked_ads: list, context_card: dict}
# Stores the ranked list of original ads for each user
ranked_ads_cache = {}

# Best variant cache: user_id -> {ad_key: best_variant_result}
# Avoids re-remixing the same original ad repeatedly
best_variant_cache = {}

# Tracks background pre-warm jobs so we don't duplicate work
# Structure: user_id -> set(ad_keys currently warming)
prewarm_inflight = {}

# Cache queue config
QUEUE_TOP_N = 10          # how many ads to keep in the temp queue
QUEUE_REFILL_THRESHOLD = 2  # when queue falls below this, try to top-up


def _build_context_card_from_dict(context_card_dict: dict, user_id: str):
    """Convert context_card dict to ContextCard instance."""
    from context_agent import ContextCard, RerankedPost

    posts = [
        RerankedPost(
            post_id=p.get("post_id", ""),
            text=p.get("text", ""),
            source=p.get("source", ""),
            rank=p.get("rank", 0),
            relevance_score=p.get("relevance_score", 0.5)
        )
        for p in context_card_dict.get("top_25_reranked_posts", [])
    ]
    return ContextCard(
        username=context_card_dict.get("username", user_id),
        user_id=context_card_dict.get("user_id", user_id),
        general_topic=context_card_dict.get("general_topic", ""),
        popular_memes=context_card_dict.get("popular_memes"),
        user_persona_tone=context_card_dict.get("user_persona_tone", ""),
        top_25_reranked_posts=posts
    )


def warm_ranked_ads(user_id: str, refresh: bool = False):
    """
    Ensure ranked ads are fetched and cached for a user.
    Returns (context_card, ranked_ads, cache_key).
    """
    from pipeline import AdIntelligencePipeline

    # If cached and not refresh, return existing (stored by user_id alias)
    if not refresh and user_id in ranked_ads_cache:
        info = ranked_ads_cache[user_id]
        cc_dict = info.get("context_card", {})
        context_card = _build_context_card_from_dict(cc_dict, user_id) if cc_dict else None
        cache_key = info.get("cache_key", user_id)
        return context_card, info.get("ranked_ads", []), cache_key

    # Step 1: get context card (from existing data or on-demand)
    context_card = None
    ad_data = load_latest_result_for_user(user_id)
    if ad_data and ad_data.get("context_card"):
        context_card = _build_context_card_from_dict(ad_data["context_card"], user_id)
    else:
        # Try to generate ads (which also builds context card)
        ad_data = generate_ads_on_demand(user_id, run_full_pipeline=True)
        if ad_data and ad_data.get("context_card"):
            context_card = _build_context_card_from_dict(ad_data["context_card"], user_id)

    if not context_card:
        return None, []

    # Step 2: fetch relevant ads and rank
    ads = fetch_relevant_ads(context_card.to_dict(), limit=50)
    pipeline = AdIntelligencePipeline(ads=ads)
    ranked_ads = pipeline.get_ranked_ads(context_card)
    cache_key = get_cache_key_for_user(user_id, context_card.to_dict())
    ranked_info = {
        "ranked_ads": ranked_ads,
        "context_card": context_card.to_dict(),
        "timestamp": datetime.now().isoformat(),
        "cache_key": cache_key
    }
    ranked_ads_cache[cache_key] = ranked_info
    ranked_ads_cache[user_id] = ranked_info  # alias for direct user_id access
    # Reset rotation and best-variant cache on rebuild
    rotation_obj = {
        "shown_keys": set(),
        "last_key": None,
        "last_index": -1
    }
    ad_rotation[cache_key] = rotation_obj
    ad_rotation[user_id] = rotation_obj  # alias for direct user_id access
    best_variant_cache.pop(cache_key, None)

    return context_card, ranked_ads, cache_key

# -------------------------------
# Temp queue helpers
# -------------------------------


def build_queue_for_user(user_id: str, refresh: bool = False, top_n: int = QUEUE_TOP_N) -> list | None:
    """
    Build and persist a fresh queue of ads for the user, respecting served history.
    Returns the queue list or None if generation failed.
    """
    from pipeline import AdIntelligencePipeline

    # Use existing served_keys to avoid repeats
    state, _ = load_queue_state(user_id)
    served_keys = state.get("served_keys", [])

    context_card, ranked_ads, cache_key = warm_ranked_ads(user_id, refresh=refresh)
    if not context_card:
        return None

    pipeline = AdIntelligencePipeline()
    username = getattr(context_card, "username", user_id) or user_id

    queue = pipeline.generate_top_ad_queue(
        context_card,
        user_id=user_id,
        username=username,
        served_keys=served_keys,
        top_n=top_n,
    )

    replace_queue(
        user_id,
        username=username,
        queue=queue,
        served_keys=served_keys,
    )
    return queue


def ensure_queue_has_items(user_id: str, refresh: bool = False) -> list | None:
    """
    Ensure the temp queue has items. If empty or refresh requested,
    rebuild from pipeline rankings and best variants.
    """
    state, _ = load_queue_state(user_id)
    queue = state.get("queue", [])
    if queue and not refresh:
        return queue
    return build_queue_for_user(user_id, refresh=refresh)


def maybe_top_up_queue(user_id: str):
    """
    If the queue is running low, top it up with fresh ads not yet served.
    """
    state, _ = load_queue_state(user_id)
    remaining = len(state.get("queue", []))
    if remaining >= QUEUE_REFILL_THRESHOLD:
        return

    try:
        from pipeline import AdIntelligencePipeline

        served_keys = state.get("served_keys", [])
        context_card, ranked_ads, cache_key = warm_ranked_ads(user_id, refresh=False)
        if not context_card:
            return

        pipeline = AdIntelligencePipeline()
        username = getattr(context_card, "username", user_id) or user_id

        new_ads = pipeline.generate_top_ad_queue(
            context_card,
            user_id=user_id,
            username=username,
            served_keys=served_keys,
            top_n=QUEUE_TOP_N,
        )

        added = append_ads_to_queue(user_id, new_ads, username=username)
        if added:
            print(f"[Queue] Topped up {added} ads for {user_id}. Queue now {queue_size(user_id)}.")
    except Exception as e:
        print(f"[Queue] Failed to top up queue for {user_id}: {e}")


def refresh_queue_from_latest(user_id: str):
    """
    Rebuild the queue immediately after a pipeline generation completes.
    """
    try:
        queue = build_queue_for_user(user_id, refresh=True, top_n=QUEUE_TOP_N)
        if queue is not None:
            print(f"[Queue] Refreshed queue for {user_id} ({len(queue)} items)")
        return queue
    except Exception as e:
        print(f"[Queue] Failed to refresh queue for {user_id}: {e}")
        return None

# Pipeline execution tracking: user_id -> {status, thread, error}
# Tracks ongoing pipeline executions
pipeline_status = {}


def get_cache_key_for_user(user_id: str, ad_data: dict = None) -> str:
    """
    Get the cache key for a user (username if available, otherwise user_id).
    Also checks ad_data if provided to extract username.
    """
    if ad_data:
        username = ad_data.get("username")
        if username:
            return username
    return user_id


def schedule_best_variant_prewarm(user_id: str, cache_key: str, context_card, ranked_ads: list, target_index: int | None = None) -> None:
    """
    Kick off a background remix of the next ad so it is cached before the
    next request. This keeps the endpoint responsive and makes caching
    truly preemptive.
    """
    if not ranked_ads:
        return

    # Decide which ad to prewarm (default: the next in rotation)
    if target_index is None:
        target_index = get_next_original_ad_index(cache_key, ranked_ads)

    if target_index is None or target_index >= len(ranked_ads):
        return

    original_ad = ranked_ads[target_index]
    ad_key = get_ad_key(original_ad)

    # Skip if already cached or currently warming
    if best_variant_cache.get(cache_key, {}).get(ad_key):
        return
    if ad_key in prewarm_inflight.get(cache_key, set()):
        return

    prewarm_inflight.setdefault(cache_key, set()).add(ad_key)
    cc_payload = context_card.to_dict() if hasattr(context_card, "to_dict") else context_card

    def _worker():
        try:
            result = remix_and_get_best_variant(user_id, cc_payload, original_ad, target_index)
            if result:
                best_variant_cache.setdefault(cache_key, {})[ad_key] = result
                print(f"[Prewarm] Cached best variant for {cache_key} ad #{target_index}")
        except Exception as e:
            print(f"[Prewarm] Error prewarming ad #{target_index} for {cache_key}: {e}")
        finally:
            inflight = prewarm_inflight.get(cache_key)
            if inflight:
                inflight.discard(ad_key)
                if not inflight:
                    prewarm_inflight.pop(cache_key, None)

    threading.Thread(target=_worker, daemon=True).start()


def get_ad_key(ad: dict) -> str:
    """Proxy to shared ad key computation."""
    return compute_ad_key(ad)


def load_latest_result_for_user(username: str) -> dict | None:
    """Load the most recent pipeline result for a user from JSON files."""
    pattern = str(OUTPUT_DIR / f"{username}_*_full_result.json")
    files = glob(pattern)
    
    if not files:
        return None
    
    # Sort by modification time, newest first
    files.sort(key=os.path.getmtime, reverse=True)
    latest_file = files[0]
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_user_data_file(username: str) -> dict | None:
    """Try to load user data JSON file for on-demand generation."""
    # Try multiple patterns
    patterns = [
        f"{username}_*.json",
        f"user_data_{username}_*.json",
        f"user_data_{username}.json"
    ]
    
    for pattern in patterns:
        files = glob(str(OUTPUT_DIR / pattern))
        if files:
            # Sort by modification time, newest first
            files.sort(key=os.path.getmtime, reverse=True)
            try:
                with open(files[0], 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # Check if it looks like user data
                    if isinstance(data, dict) and ('posts' in data or 'username' in data):
                        return data
            except Exception:
                continue
    
    # Also check runs directory
    runs_dir = OUTPUT_DIR / "runs"
    if runs_dir.exists():
        for pattern in patterns:
            files = glob(str(runs_dir / "**" / pattern), recursive=True)
            if files:
                files.sort(key=os.path.getmtime, reverse=True)
                try:
                    with open(files[0], 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, dict) and ('posts' in data or 'username' in data):
                            return data
                except Exception:
                    continue
    
    return None


def fetch_user_data_from_x_api(username: str) -> dict | None:
    """
    Fetch user data from X API using stored tokens.
    
    Requires OAuth token stored via /api/auth/token endpoint.
    For full data (posts, likes, bookmarks), OAuth is required.
    
    Returns:
        User data dict if successful, None otherwise
    """
    try:
        import requests
        
        # Try to use stored access token if available
        token_file = OUTPUT_DIR / f".token_{username}.json"
        access_token = None
        
        if token_file.exists():
            try:
                with open(token_file, 'r') as f:
                    token_data = json.load(f)
                    access_token = token_data.get("access_token")
            except Exception:
                pass
        
        # If we have an access token, use it
        if access_token:
            try:
                from auth_client import AuthClient, UserData
                
                # Create auth client and set token
                client = AuthClient()
                client.access_token = access_token
                
                # Create xdk client with token
                from xdk import Client
                client.xdk_client = Client(bearer_token=access_token)
                
                # Get user info
                user_id, fetched_username = client.get_authenticated_user()
                
                # Fetch user data using the auth client's method
                headers = {"Authorization": f"Bearer {access_token}"}
                base_params = {
                    "max_results": 25,
                    "tweet.fields": "created_at,public_metrics,text,author_id"
                }
                
                user_data = UserData(
                    user_id=user_id,
                    username=fetched_username or username,
                    posts=[],
                    timeline=[],
                    likes=[],
                    bookmarks=[]
                )
                
                # Fetch Posts
                posts_url = f"https://api.x.com/2/users/{user_id}/tweets"
                posts_resp = requests.get(posts_url, headers=headers, params=base_params)
                if posts_resp.status_code == 200:
                    user_data.posts = posts_resp.json().get('data', [])
                
                # Fetch Timeline
                timeline_url = f"https://api.x.com/2/users/{user_id}/timelines/reverse_chronological"
                timeline_resp = requests.get(timeline_url, headers=headers, params=base_params)
                if timeline_resp.status_code == 200:
                    user_data.timeline = timeline_resp.json().get('data', [])
                
                # Fetch Likes
                likes_url = f"https://api.x.com/2/users/{user_id}/liked_tweets"
                likes_resp = requests.get(likes_url, headers=headers, params=base_params)
                if likes_resp.status_code == 200:
                    user_data.likes = likes_resp.json().get('data', [])
                
                # Fetch Bookmarks
                bookmarks_url = f"https://api.x.com/2/users/{user_id}/bookmarks"
                bookmarks_resp = requests.get(bookmarks_url, headers=headers, params=base_params)
                if bookmarks_resp.status_code == 200:
                    user_data.bookmarks = bookmarks_resp.json().get('data', [])
                
                print(f"[X API] Successfully fetched data for {username}: {len(user_data.posts)} posts, {len(user_data.likes)} likes")
                return user_data.to_dict()
                
            except Exception as e:
                print(f"[X API] Token-based fetch failed: {e}")
                import traceback
                traceback.print_exc()
        
        # No token available
        print(f"[X API] No stored token for {username}, cannot fetch without OAuth")
        return None
        
    except Exception as e:
        print(f"[X API] Error fetching user data: {e}")
        import traceback
        traceback.print_exc()
        return None


def run_full_pipeline_async(user_id: str, user_data: dict = None):
    """
    Run the full pipeline asynchronously in a background thread.
    
    Args:
        user_id: User identifier
        user_data: Optional user data dict (if None, will try to fetch)
    """
    def pipeline_worker():
        try:
            from pipeline import AdIntelligencePipeline, save_results
            
            pipeline_status[user_id] = {
                "status": "running",
                "started_at": datetime.now().isoformat(),
                "error": None
            }
            
            print(f"[Pipeline] Starting full pipeline for {user_id}...")
            
            # Get user data if not provided
            if not user_data:
                # Try loading from file first
                data = load_user_data_file(user_id)
                if not data:
                    # Try fetching from X API
                    data = fetch_user_data_from_x_api(user_id)
                
                if not data:
                    raise ValueError(f"Could not fetch user data for {user_id}")
            else:
                data = user_data
            
            # Run pipeline
            pipeline = AdIntelligencePipeline()
            result = pipeline.run_from_user_data(data)
            
            # Save results
            save_results(result, str(OUTPUT_DIR))
            
            # Update cache
            ad_dict = result.to_dict()
            cache_key = get_cache_key_for_user(user_id, ad_dict)
            ads_cache[cache_key] = {
                "data": ad_dict,
                "timestamp": datetime.now().isoformat()
            }
            if cache_key != user_id:
                ads_cache[user_id] = ads_cache[cache_key]
            
            # Refresh temp queue so the extension gets fresh ads immediately
            refresh_queue_from_latest(cache_key)

            # Initialize rotation tracking
            if cache_key not in ad_rotation:
                ad_rotation[cache_key] = {
                    "shown_indices": set(),
                    "last_index": -1
                }
            
            pipeline_status[user_id] = {
                "status": "completed",
                "started_at": pipeline_status[user_id]["started_at"],
                "completed_at": datetime.now().isoformat(),
                "error": None
            }
            
            print(f"[Pipeline] Successfully completed pipeline for {user_id}")
            
        except Exception as e:
            error_msg = str(e)
            print(f"[Pipeline] Error in pipeline for {user_id}: {error_msg}")
            pipeline_status[user_id] = {
                "status": "failed",
                "started_at": pipeline_status.get(user_id, {}).get("started_at", datetime.now().isoformat()),
                "error": error_msg
            }
    
    # Start pipeline in background thread
    thread = threading.Thread(target=pipeline_worker, daemon=True)
    thread.start()
    
    pipeline_status[user_id] = {
        "status": "starting",
        "started_at": datetime.now().isoformat(),
        "thread": thread
    }


def generate_ads_on_demand(user_id: str, run_full_pipeline: bool = True) -> dict | None:
    """
    Try to generate ads on-demand for a user.
    
    Args:
        user_id: User identifier
        run_full_pipeline: If True, run full pipeline; if False, only try loading files
    
    Returns:
        Ad data dict if successful, None otherwise
    """
    # First, try loading existing results
    ad_data = load_latest_result_for_user(user_id)
    if ad_data:
        cache_key = get_cache_key_for_user(user_id, ad_data)
        ads_cache[cache_key] = {
            "data": ad_data,
            "timestamp": datetime.now().isoformat()
        }
        if cache_key != user_id:
            ads_cache[user_id] = ads_cache[cache_key]
        return ad_data
    
    # If run_full_pipeline is False, don't try to generate
    if not run_full_pipeline:
        return None
    
    # Check if pipeline is already running
    if user_id in pipeline_status:
        status = pipeline_status[user_id].get("status")
        if status in ["starting", "running"]:
            print(f"[On-demand] Pipeline already running for {user_id}")
            return None
        elif status == "failed":
            # Clear failed status and retry
            del pipeline_status[user_id]
    
    # Try to load user data from files
    user_data = load_user_data_file(user_id)
    
    if user_data:
        # Run pipeline synchronously (will take time but ensures we have data)
        try:
            from pipeline import AdIntelligencePipeline, save_results
            
            print(f"[On-demand] Running full pipeline for {user_id}...")
            pipeline = AdIntelligencePipeline()
            result = pipeline.run_from_user_data(user_data)
            
            # Save results
            save_results(result, str(OUTPUT_DIR))
            
            # Update cache
            ad_dict = result.to_dict()
            cache_key = get_cache_key_for_user(user_id, ad_dict)
            ads_cache[cache_key] = {
                "data": ad_dict,
                "timestamp": datetime.now().isoformat()
            }
            if cache_key != user_id:
                ads_cache[user_id] = ads_cache[cache_key]
            
            # Refresh queue with the new generation
            refresh_queue_from_latest(cache_key)

            # Initialize rotation tracking
            if cache_key not in ad_rotation:
                ad_rotation[cache_key] = {
                    "shown_indices": set(),
                    "last_index": -1
                }
            
            print(f"[On-demand] Successfully generated ads for {user_id}")
            return ad_dict
            
        except Exception as e:
            print(f"[On-demand] Error generating ads for {user_id}: {e}")
            # Try async execution as fallback
            run_full_pipeline_async(user_id, user_data)
            return None
    else:
        # Try fetching from X API and run async
        print(f"[On-demand] No user data file found, attempting X API fetch for {user_id}...")
        user_data = fetch_user_data_from_x_api(user_id)
        if user_data:
            run_full_pipeline_async(user_id, user_data)
        else:
            print(f"[On-demand] Cannot fetch user data for {user_id} without OAuth")
    
    return None


def get_next_original_ad_index(user_id: str, ranked_ads: list) -> int:
    """
    Get the index of the next original ad to remix and serve.
    Tracks which original ads have been served (not variants) using ad keys.
    
    Args:
        user_id: User identifier
        ranked_ads: List of ranked original ads
        
    Returns:
        Index of next original ad to remix, or None if all served
    """
    if not ranked_ads:
        return 0

    total_ads = len(ranked_ads)
    # If there's only one ad, always serve that one (avoid resetting)
    if total_ads <= 1:
        return 0

    if user_id not in ad_rotation:
        ad_rotation[user_id] = {
            "shown_keys": set(),
            "last_key": None,
            "last_index": -1
        }
    
    rotation = ad_rotation[user_id]
    # Normalize legacy structures that used shown_indices
    if "shown_keys" not in rotation and "shown_indices" in rotation:
        rotation["shown_keys"] = set(rotation.get("shown_indices", set()))
    shown_keys = rotation["shown_keys"]

    # Build ad keys list for ranked ads
    ad_keys = []
    for ad in ranked_ads:
        ad_keys.append(get_ad_key(ad))

    # If all original ads have been shown, reset and start over
    if len(shown_keys) >= total_ads:
        shown_keys.clear()
        rotation["last_key"] = None
        rotation["last_index"] = -1

    # Find next unshown ad by key (they're already ranked, so pick first unshown)
    for i, key in enumerate(ad_keys):
        if key not in shown_keys:
            return i

    # Fallback: return 0
    return 0


def remix_and_get_best_variant(user_id: str, context_card: dict, original_ad: dict, ad_index: int) -> dict:
    """
    Remix a single original ad and return only the best variant.
    
    Args:
        user_id: User identifier
        context_card: User context card
        original_ad: Original ad dict to remix
        ad_index: Index of this ad in ranked list
        
    Returns:
        Dict with best variant ready to serve
    """
    try:
        from pipeline import AdIntelligencePipeline
        from context_agent import ContextCard
        
        # Convert context_card dict to ContextCard if needed
        if isinstance(context_card, dict):
            from context_agent import RerankedPost
            posts = [
                RerankedPost(
                    post_id=p.get("post_id", ""),
                    text=p.get("text", ""),
                    source=p.get("source", ""),
                    rank=p.get("rank", 0),
                    relevance_score=p.get("relevance_score", 0.5)
                )
                for p in context_card.get("top_25_reranked_posts", [])
            ]
            cc = ContextCard(
                username=context_card.get("username", user_id),
                user_id=context_card.get("user_id", user_id),
                general_topic=context_card.get("general_topic", ""),
                popular_memes=context_card.get("popular_memes"),
                user_persona_tone=context_card.get("user_persona_tone", ""),
                top_25_reranked_posts=posts
            )
        else:
            cc = context_card
        
        pipeline = AdIntelligencePipeline()
        result = pipeline.remix_single_ad_and_get_best(cc, original_ad, ad_index)
        
        return result
        
    except Exception as e:
        print(f"[Remix] Error remixing ad: {e}")
        import traceback
        traceback.print_exc()
        return None


def mark_original_ad_as_shown(user_key: str, original_ad_index: int):
    """Mark an original ad as shown (after serving its best variant)."""
    if user_key not in ad_rotation:
        ad_rotation[user_key] = {
            "shown_keys": set(),
            "last_key": None,
            "last_index": -1
        }
    rotation = ad_rotation[user_key]
    if "shown_keys" not in rotation and "shown_indices" in rotation:
        rotation["shown_keys"] = set(rotation.get("shown_indices", set()))
    
    # Fetch current ranked ads to map index -> key
    ranked_info = ranked_ads_cache.get(user_key) or ranked_ads_cache.get(get_cache_key_for_user(user_key))
    ad_key = None
    if ranked_info:
        ranked_ads = ranked_info.get("ranked_ads", [])
        if 0 <= original_ad_index < len(ranked_ads):
            ad_key = get_ad_key(ranked_ads[original_ad_index])
    if ad_key is None:
        ad_key = f"ad_index_{original_ad_index}"

    rotation["shown_keys"].add(ad_key)
    rotation["last_key"] = ad_key
    rotation["last_index"] = original_ad_index


def format_best_variant_for_extension(best_variant_result: dict, user_id: str) -> dict:
    """
    Format the best variant result for the Chrome extension.
    
    Args:
        best_variant_result: Result from remix_single_ad_and_get_best
        user_id: User identifier
    """
    best_variant = best_variant_result["best_variant"]
    content = best_variant.content

    # Prefer the winning image, then enhanced, then original
    image_uri = (
        getattr(best_variant, "image_uri", None)
        or getattr(best_variant, "enhanced_image_uri", None)
        or getattr(best_variant, "original_image_uri", None)
    )
    
    # Parse content to extract title (first line) and description
    lines = content.split('\n')
    title = lines[0].strip() if lines else "Sponsored Ad"
    description = lines[1].strip() if len(lines) > 1 else ""
    
    return {
        "id": f"{user_id}_ad_{best_variant_result['ad_index']}",
        "title": title,
        "description": description,     
        "full_content": content,
        "image_uri": best_variant.image_uri,
        "brand": "xAI AdBot",
        "avatar": "https://images.seeklogo.com/logo-png/49/1/twitter-x-logo-png_seeklogo-492396.png",
        "ctr_score": best_variant_result["ctr_score"],
        "confidence": best_variant_result["confidence"],
        "ad_index": best_variant_result["ad_index"],
        "original_ad": best_variant_result["original_ad"],
        "ad_key": best_variant_result.get("ad_key")
    }


def format_ad_for_extension(ad_data: dict, index: int = 0, use_best: bool = False) -> dict:
    """
    Format ad data for the Chrome extension.
    
    Args:
        ad_data: Ad data dictionary
        index: Specific ad index to format (used when use_best=False)
        use_best: If True, use best ad from CTR prediction; otherwise use index
    """
    rewritten_ads = ad_data.get("remixed_ads", {}).get("rewritten_ads", [])
    ctr_prediction = ad_data.get("ctr_prediction", {})
    
    if not rewritten_ads:
        return None
    
    # Determine which ad index to use
    if use_best:
        ad_index = ctr_prediction.get("best_ad_index", index)
    else:
        ad_index = index
    
    # Ensure index is valid
    if ad_index >= len(rewritten_ads):
        ad_index = 0
    
    ad = rewritten_ads[ad_index]
    content = ad.get("content", "")
    
    # Parse content to extract title (first line) and description
    lines = content.split('\n')
    title = lines[0].strip() if lines else "Sponsored Ad"
    description = lines[1].strip() if len(lines) > 1 else ""
    
    # Get CTR score for this specific ad
    scores = ctr_prediction.get("scores", [])
    score_data = next((s for s in scores if s.get("ad_index") == ad_index), {})
    ctr_score = score_data.get("ctr_mean", 0)
    confidence = score_data.get("ctr_std", ctr_prediction.get("confidence", 0))
    
    return {
        "id": f"{ad_data.get('user_id', 'unknown')}_{ad_index}",
        "title": title,
        "description": description,
        "full_content": content,
        "image_uri": ad.get("image_uri"),
        "brand": "xAI AdBot",
        "avatar": "https://w7.pngwing.com/pngs/676/1/png-transparent-x-icon-ex-twitter-tech-companies-social-media.png",
        "ctr_score": ctr_score,
        "confidence": confidence,
        "ad_index": ad_index,
        "total_variants": len(rewritten_ads)
    }


def format_all_ads_for_extension(ad_data: dict) -> list:
    """Format all ad variants for the Chrome extension."""
    rewritten_ads = ad_data.get("remixed_ads", {}).get("rewritten_ads", [])
    ctr_prediction = ad_data.get("ctr_prediction", {})
    scores = ctr_prediction.get("scores", [])
    
    formatted_ads = []
    for i, ad in enumerate(rewritten_ads):
        content = ad.get("content", "")
        lines = content.split('\n')
        title = lines[0].strip() if lines else "Sponsored Ad"
        description = lines[1].strip() if len(lines) > 1 else ""
        
        # Find score for this ad
        score_data = next((s for s in scores if s.get("ad_index") == i), {})
        
        formatted_ads.append({
            "id": f"{ad_data.get('user_id', 'unknown')}_{i}",
            "title": title,
            "description": description,
            "full_content": content,
            "image_uri": ad.get("image_uri"),
            "brand": "xAI AdBot",
            "avatar": "https://w7.pngwing.com/pngs/676/1/png-transparent-x-icon-ex-twitter-tech-companies-social-media.png",
            "ctr_score": score_data.get("ctr_mean", 0),
            "confidence": score_data.get("ctr_std", 0),
            "ad_index": i,
            "is_best": i == ctr_prediction.get("best_ad_index", 0)
        })
    
    # Sort by CTR score descending
    formatted_ads.sort(key=lambda x: x["ctr_score"], reverse=True)
    
    return formatted_ads


@app.route("/health")
def health():
    """Health check endpoint."""
    return jsonify({"status": "ok", "timestamp": datetime.now().isoformat()})


@app.route("/api/users")
def list_users():
    """List all users with available ads."""
    pattern = str(OUTPUT_DIR / "*_full_result.json")
    files = glob(pattern)
    
    users = set()
    for f in files:
        filename = os.path.basename(f)
        # Extract username from filename (format: username_timestamp_full_result.json)
        parts = filename.split('_')
        if len(parts) >= 3:
            users.add(parts[0])
    
    return jsonify({
        "users": list(users),
        "cached_users": list(ads_cache.keys())
    })


@app.route("/api/ad/<user_id>")
def get_ad(user_id: str):
    """
    Serve the next ad for a user from the temp cache queue.

    Flow:
    1. Ensure a queue exists (build if empty or refresh requested)
    2. Pop the next ad from the queue (removes it from cache)
    3. If queue is low, top-up with fresh ads the user hasn't seen
    
    Query params:
        - refresh: bool (optional) - force rebuild queue/context
    """
    refresh = request.args.get("refresh", "false").lower() == "true"
    
    try:
        # Ensure queue exists (build if empty or refresh requested)
        queue = ensure_queue_has_items(user_id, refresh=refresh)
        if not queue:
            # Check if pipeline is running
            if user_id in pipeline_status:
                status_info = pipeline_status[user_id]
                if status_info.get("status") in ["starting", "running"]:
                    return jsonify({
                        "status": "generating",
                        "message": "Pipeline is running, please check back shortly",
                        "user_id": user_id,
                        "started_at": status_info.get("started_at")
                    }), 202
            return jsonify({
                "error": "No ads available. Run pipeline.py to generate ads first.",
                "user_id": user_id
            }), 404

        # Pop next ad (this also marks it served)
        ad, state = pop_next_ad(user_id)

        # If queue was empty after pop, try a forced rebuild once
        if not ad:
            queue = ensure_queue_has_items(user_id, refresh=True)
            if queue:
                ad, state = pop_next_ad(user_id)

        if not ad:
            return jsonify({
                "error": "No ads available after refill",
                "user_id": user_id
            }), 404

        # Top-up queue asynchronously-safe (best effort)
        maybe_top_up_queue(user_id)

        return jsonify(ad)

    except Exception as e:
        print(f"[Ad Server] Error in get_ad: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "error": str(e),
            "type": type(e).__name__
        }), 500


@app.route("/api/ads/<user_id>")
def get_all_ads(user_id: str):
    """Get all ad variants for a user, sorted by CTR score."""
    refresh = request.args.get("refresh", "false").lower() == "true"
    
    # Check cache first
    if not refresh and user_id in ads_cache:
        ad_data = ads_cache[user_id]["data"]
    else:
        ad_data = load_latest_result_for_user(user_id)
        
        if ad_data:
            ads_cache[user_id] = {
                "data": ad_data,
                "timestamp": datetime.now().isoformat()
            }
    
    # If no ads found, try on-demand generation
    if not ad_data:
        ad_data = generate_ads_on_demand(user_id, run_full_pipeline=True)
        
        if not ad_data:
            # Check if pipeline is running
            if user_id in pipeline_status:
                status_info = pipeline_status[user_id]
                if status_info.get("status") in ["starting", "running"]:
                    return jsonify({
                        "status": "generating",
                        "message": "Pipeline is running, please check back shortly",
                        "user_id": user_id,
                        "started_at": status_info.get("started_at")
                    }), 202
            
            return jsonify({
                "error": "No ads found for user",
                "user_id": user_id
            }), 404
    
    formatted = format_all_ads_for_extension(ad_data)
    
    # Add rotation status to response
    rotation_status = None
    cache_key = get_cache_key_for_user(user_id, ad_data or {})
    rotation = ad_rotation.get(cache_key) or ad_rotation.get(user_id)
    if rotation:
        rotation_status = {
            "shown_count": len(rotation.get("shown_indices", set()) or rotation.get("shown_keys", set())),
            "shown_indices": sorted(list(rotation.get("shown_indices", set()))),
            "last_shown": rotation.get("last_index")
        }
    
    return jsonify({
        "user_id": user_id,
        "username": ad_data.get("username", user_id),
        "total_ads": len(formatted),
        "ads": formatted,
        "context": {
            "topic": ad_data.get("context_card", {}).get("general_topic", ""),
            "tone": ad_data.get("context_card", {}).get("user_persona_tone", "")
        },
        "rotation": rotation_status
    })


@app.route("/api/generate/<user_id>", methods=["POST"])
def generate_ads(user_id: str):
    """
    Trigger the pipeline to generate new ads for a user.
    
    Request body (JSON):
        - context_card: dict (optional) - existing context card to use
        - user_data: dict (optional) - user data if no context card
        - auto_load: bool (optional, default true) - try to load user_data from files if not provided
    """
    try:
        from pipeline import AdIntelligencePipeline, save_results
        
        data = request.get_json() or {}
        auto_load = data.get("auto_load", True)
        
        pipeline = AdIntelligencePipeline()
        
        if "context_card" in data:
            result = pipeline.run_from_context_card(data["context_card"])
        elif "user_data" in data:
            result = pipeline.run_from_user_data(data["user_data"])
        elif auto_load:
            # Try to load user data automatically
            user_data = load_user_data_file(user_id)
            if user_data:
                result = pipeline.run_from_user_data(user_data)
            else:
                return jsonify({
                    "error": "No user data found. Provide context_card or user_data in request body.",
                    "user_id": user_id
                }), 400
        else:
            return jsonify({
                "error": "Must provide either context_card or user_data"
            }), 400
        
        # Save results
        save_results(result, str(OUTPUT_DIR))
        
        # Update cache
        cache_key = result.username or user_id
        ads_cache[cache_key] = {
            "data": result.to_dict(),
            "timestamp": datetime.now().isoformat()
        }
        
        # Refresh queue immediately with the new generation
        refresh_queue_from_latest(cache_key)

        # Reset rotation tracking for this user
        if cache_key in ad_rotation:
            ad_rotation[cache_key] = {
                "shown_indices": set(),
                "last_index": -1
            }
        
        # Return the best ad
        formatted = format_ad_for_extension(result.to_dict())
        
        return jsonify({
            "success": True,
            "user_id": result.user_id,
            "username": result.username,
            "best_ad": formatted,
            "all_ads": format_all_ads_for_extension(result.to_dict())
        })
        
    except Exception as e:
        return jsonify({
            "error": str(e),
            "type": type(e).__name__
        }), 500


@app.route("/api/ad/<user_id>/mark-shown", methods=["POST"])
def mark_ad_shown(user_id: str):
    """
    Mark an ad as shown for rotation tracking.
    
    Request body (JSON):
        - ad_index: int (required) - index of the ad that was shown
    """
    data = request.get_json() or {}
    ad_index = data.get("ad_index")
    
    if ad_index is None:
        return jsonify({
            "error": "ad_index is required"
        }), 400
    
    mark_original_ad_as_shown(user_id, ad_index)
    
    rotation = ad_rotation.get(user_id, {})
    return jsonify({
        "success": True,
        "user_id": user_id,
        "ad_index": ad_index,
        "shown_count": len(rotation.get("shown_indices", set())),
        "shown_indices": sorted(list(rotation.get("shown_indices", set())))
    })


@app.route("/api/ad/<user_id>/rotation", methods=["GET"])
def get_rotation_status(user_id: str):
    """Get rotation status for a user."""
    if user_id not in ad_rotation:
        return jsonify({
            "user_id": user_id,
            "shown_count": 0,
            "shown_indices": [],
            "last_shown": -1
        })
    
    rotation = ad_rotation[user_id]
    return jsonify({
        "user_id": user_id,
        "shown_count": len(rotation["shown_indices"]),
        "shown_indices": sorted(list(rotation["shown_indices"])),
        "last_shown": rotation["last_index"]
    })


@app.route("/api/pipeline/<user_id>/status", methods=["GET"])
def get_pipeline_status(user_id: str):
    """Get the status of pipeline execution for a user."""
    if user_id not in pipeline_status:
        return jsonify({
            "user_id": user_id,
            "status": "not_started"
        })
    
    return jsonify(pipeline_status[user_id])


@app.route("/api/auth/token", methods=["POST"])
def store_auth_token():
    """
    Store OAuth access token for a user to enable automatic data fetching.
    
    Request body (JSON):
        - username: str (required) - Username to associate token with
        - access_token: str (required) - OAuth access token
        - refresh_token: str (optional) - Refresh token if available
    """
    data = request.get_json() or {}
    username = data.get("username")
    access_token = data.get("access_token")
    
    if not username or not access_token:
        return jsonify({
            "error": "username and access_token are required"
        }), 400
    
    # Store token securely (in production, use proper encryption)
    token_file = OUTPUT_DIR / f".token_{username}.json"
    token_data = {
        "username": username,
        "access_token": access_token,
        "refresh_token": data.get("refresh_token"),
        "stored_at": datetime.now().isoformat()
    }
    
    try:
        with open(token_file, 'w') as f:
            json.dump(token_data, f, indent=2)
        
        return jsonify({
            "success": True,
            "message": f"Token stored for {username}",
            "username": username
        })
    except Exception as e:
        return jsonify({
            "error": f"Failed to store token: {str(e)}"
        }), 500


@app.route("/api/prefetch/<user_id>", methods=["POST", "GET"])
def prefetch_ads(user_id: str):
    """
    Prefetch context card and ranked ads for a user (run on extension start).
    Query params / body:
        - refresh: bool (optional) - force rebuild of context/ranking
    """
    refresh = False
    if request.method == "POST":
        data = request.get_json() or {}
        refresh = data.get("refresh", False)
    else:
        refresh = request.args.get("refresh", "false").lower() == "true"

    try:
        context_card, ranked_ads, cache_key = warm_ranked_ads(user_id, refresh=refresh)
        if not context_card:
            # Check if pipeline is running
            if user_id in pipeline_status:
                status_info = pipeline_status[user_id]
                if status_info.get("status") in ["starting", "running"]:
                    return jsonify({
                        "status": "generating",
                        "message": "Pipeline is running, please check back shortly",
                        "user_id": user_id,
                        "started_at": status_info.get("started_at")
                    }), 202
            return jsonify({
                "error": "No context card found for user",
                "user_id": user_id
            }), 404

        # Preemptively warm the first ad so the first impression is instant
        schedule_best_variant_prewarm(user_id, cache_key, context_card, ranked_ads, target_index=0)

        return jsonify({
            "success": True,
            "user_id": user_id,
            "ranked_ads_count": len(ranked_ads),
            "context_ready": True,
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        print(f"[Prefetch] Error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "error": str(e),
            "type": type(e).__name__
        }), 500


@app.route("/api/pipeline/<user_id>/trigger", methods=["POST"])
def trigger_pipeline(user_id: str):
    """
    Trigger full pipeline execution for a user.
    
    Request body (JSON, optional):
        - user_data: dict - User data to use (if not provided, will try to fetch)
        - async: bool (default true) - Run asynchronously
    """
    data = request.get_json() or {}
    user_data = data.get("user_data")
    run_async = data.get("async", True)
    
    # Check if already running
    if user_id in pipeline_status:
        status = pipeline_status[user_id].get("status")
        if status in ["starting", "running"]:
            return jsonify({
                "status": "already_running",
                "message": "Pipeline is already running for this user",
                "user_id": user_id,
                "started_at": pipeline_status[user_id].get("started_at")
            }), 409
    
    if run_async:
        # Run in background
        run_full_pipeline_async(user_id, user_data)
        return jsonify({
            "status": "started",
            "message": "Pipeline started in background",
            "user_id": user_id,
            "started_at": pipeline_status.get(user_id, {}).get("started_at")
        }), 202
    else:
        # Run synchronously (will block)
        try:
            from pipeline import AdIntelligencePipeline, save_results
            
            # Get user data if not provided
            if not user_data:
                user_data = load_user_data_file(user_id)
                if not user_data:
                    user_data = fetch_user_data_from_x_api(user_id)
            
            if not user_data:
                return jsonify({
                    "error": "Could not fetch user data. Provide user_data in request body or ensure user data files exist."
                }), 400
            
            pipeline = AdIntelligencePipeline()
            result = pipeline.run_from_user_data(user_data)
            
            # Save results
            save_results(result, str(OUTPUT_DIR))
            
            # Update cache
            ad_dict = result.to_dict()
            cache_key = get_cache_key_for_user(user_id, ad_dict)
            ads_cache[cache_key] = {
                "data": ad_dict,
                "timestamp": datetime.now().isoformat()
            }
            if cache_key != user_id:
                ads_cache[user_id] = ads_cache[cache_key]
            
            return jsonify({
                "status": "completed",
                "user_id": result.user_id,
                "username": result.username,
                "message": "Pipeline completed successfully"
            })
            
        except Exception as e:
            return jsonify({
                "error": str(e),
                "type": type(e).__name__
            }), 500


@app.route("/api/ad/random")
def get_random_ad():
    """Get a random ad from any available user (for testing)."""
    pattern = str(OUTPUT_DIR / "*_full_result.json")
    files = glob(pattern)
    
    if not files:
        return jsonify({"error": "No ads available"}), 404
    
    import random
    random_file = random.choice(files)
    
    with open(random_file, 'r', encoding='utf-8') as f:
        ad_data = json.load(f)
    
    formatted = format_ad_for_extension(ad_data)
    return jsonify(formatted)


def get_available_users() -> list:
    """Get list of users with available ads (helper for startup)."""
    pattern = str(OUTPUT_DIR / "*_full_result.json")
    files = glob(pattern)
    
    users = set()
    for f in files:
        filename = os.path.basename(f)
        parts = filename.split('_')
        if len(parts) >= 3:
            users.add(parts[0])
    
    return list(users)


if __name__ == "__main__":
    print("\n" + "="*60)
    print("AD SERVING API SERVER")
    print("="*60)
    print(f"\nServing ads from: {OUTPUT_DIR}")
    print("\nEndpoints:")
    print("  GET  /api/ad/<user_id>                  - Get next ad (with rotation)")
    print("  GET  /api/ads/<user_id>                 - Get all ad variants")
    print("  POST /api/generate/<user_id>            - Generate new ads")
    print("  POST /api/pipeline/<user_id>/trigger    - Trigger full pipeline")
    print("  GET  /api/pipeline/<user_id>/status      - Get pipeline status")
    print("  POST /api/auth/token                      - Store OAuth token")
    print("  POST /api/ad/<user_id>/mark-shown       - Mark ad as shown")
    print("  GET  /api/ad/<user_id>/rotation         - Get rotation status")
    print("  GET  /api/users                         - List available users")
    print("  GET  /api/ad/random                      - Get random ad (testing)")
    print("  GET  /health                             - Health check")
    print("\n" + "="*60 + "\n")
    
    # List available users on startup
    users = get_available_users()
    if users:
        print(f"Available users: {', '.join(users)}")
    else:
        print("No cached ads found. Run pipeline.py to generate ads.")
    
    print("\nStarting server on http://127.0.0.1:8001")
    print("Press Ctrl+C to stop\n")
    
    app.run(host="127.0.0.1", port=8001, debug=True)
